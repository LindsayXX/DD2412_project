{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024 684\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "#open a random image and check its size\n",
    "im = Image.open(\"/Users/stella/Downloads/Animals_with_Attributes2_3/JPEGImages/hippopotamus/hippopotamus_10003.jpg\")\n",
    "#display.display(Image.open(str('/Users/stella/Downloads/Animals_with_Attributes2_3/JPEGImages/hippopotamus/hippopotamus_10003.jpg')))\n",
    "width, height = im.size \n",
    "print(width,height)\n",
    "\n",
    "#resizing\n",
    "im=im.resize((448,448),resample=0)\n",
    "#im.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the files as a tf.data.Dataset first create a dataset of the file paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://www.tensorflow.org/tutorials/load_data/images\n",
    "def image_paths(dataset_path):\n",
    "    \"\"\"\n",
    "    input: dpath of the whole image dataset\n",
    "    returns: all of the imagepaths and all of the labels/classes\n",
    "    as type tensorflow.python.framework.ops.EagerTensor\n",
    "    \"\"\"\n",
    "    \n",
    "    imagepaths=[]\n",
    "    #os.walk returns a tuple of three elements: (root_dir_path, sub_dirs, files)\n",
    "    \n",
    "    #picking up the names of the subfiles equals the names of the classes \n",
    "    classes = os.walk(dataset_path).__next__()[1]\n",
    "    \n",
    "    # List each sub-directory (the classes)\n",
    "    for c in classes:\n",
    "        #seperate animal folder(=class)\n",
    "        c_dir = os.path.join(dataset_path, c)\n",
    "        #make a walk object for each folder\n",
    "        walk = os.walk(c_dir).__next__()\n",
    "        # Add each image (walk[2]) to the imagepaths list\n",
    "        for sample in walk[2]:\n",
    "            imagepath = os.path.join(c_dir,sample)\n",
    "            imagepaths.append(imagepath)\n",
    "            \n",
    "            \n",
    "    #cpnvert to tensfor  (No, not yet!)     \n",
    "    #imagepaths = tf.convert_to_tensor(imagepaths, dtype=tf.string)\n",
    "    \n",
    "    \n",
    "    return imagepaths     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 37322 images in my dataset\n"
     ]
    }
   ],
   "source": [
    "imagepaths= image_paths('/Users/stella/Downloads/Animals_with_Attributes2_3/JPEGImages')\n",
    "print(f'I have {len(imagepaths)} images in my dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a short pure-tensorflow function that converts a file paths to an (image_data, label) pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://www.tensorflow.org/tutorials/load_data/images\n",
    "#get the label of one image\n",
    "\n",
    "def get_label(file_path):\n",
    "  # convert the path to a list of path components\n",
    "  parts = tf.strings.split(file_path, '/')\n",
    "  # The second to last is the class-directory\n",
    "  return parts[-2] \n",
    "\n",
    "#get_label('/Users/stella/Downloads/Animals_with_Attributes2_3/JPEGImages/hippopotamus/hippopotamus_10003.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://www.tensorflow.org/tutorials/load_data/images\n",
    "#decode one image\n",
    "\n",
    "[IMG_WIDTH, IMG_HEIGHT]=[448,448]\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "  return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##source: https://www.tensorflow.org/tutorials/load_data/images\n",
    "#return img,label for one image\n",
    "\n",
    "def process_path(file_path):\n",
    "  label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "  img = tf.io.read_file(file_path)\n",
    "  img = decode_img(img)\n",
    "  return img, label\n",
    "\n",
    "#process_path('/Users/stella/Downloads/Animals_with_Attributes2_3/JPEGImages/hippopotamus/hippopotamus_10003.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.DatasetV1Adapter"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://www.tensorflow.org/tutorials/load_data/images\n",
    "# Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "list_ds = tf.data.Dataset.list_files(str('/Users/stella/Downloads/Animals_with_Attributes2_3/JPEGImages/*/*'))\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "type(labeled_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((448, 448, 3), ()), types: (tf.float32, tf.string)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  [[[0.56447834 0.54140407 0.54901963]\n",
      "  [0.56966037 0.5532913  0.55926126]\n",
      "  [0.5810399  0.56029415 0.5759804 ]\n",
      "  ...\n",
      "  [0.43744785 0.28842822 0.28450665]\n",
      "  [0.45933136 0.32002816 0.3128678 ]\n",
      "  [0.47382733 0.3365724  0.32872927]]\n",
      "\n",
      " [[0.56619793 0.5344065  0.5449851 ]\n",
      "  [0.590005   0.56874156 0.57715863]\n",
      "  [0.5702327  0.5494869  0.56517315]\n",
      "  ...\n",
      "  [0.46162835 0.31088918 0.30782738]\n",
      "  [0.47517842 0.32853368 0.32382047]\n",
      "  [0.4830603  0.33691618 0.3320361 ]]\n",
      "\n",
      " [[0.5885111  0.55389977 0.5654369 ]\n",
      "  [0.5869787  0.5637274  0.57293606]\n",
      "  [0.526626   0.5022074  0.51856804]\n",
      "  ...\n",
      "  [0.4711001  0.31650528 0.3107232 ]\n",
      "  [0.48646763 0.33088046 0.32463482]\n",
      "  [0.49739763 0.34140587 0.33516026]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.19102599 0.21760584 0.23096812]\n",
      "  [0.18901306 0.21437904 0.22814594]\n",
      "  [0.21321689 0.23147596 0.24783662]\n",
      "  ...\n",
      "  [0.11891741 0.36789265 0.09993953]\n",
      "  [0.12665927 0.366174   0.12231246]\n",
      "  [0.14414571 0.3765649  0.15567482]]\n",
      "\n",
      " [[0.18592358 0.20372546 0.22031471]\n",
      "  [0.16548762 0.18509546 0.20078173]\n",
      "  [0.188622   0.20822984 0.22391611]\n",
      "  ...\n",
      "  [0.21136597 0.44871956 0.16900301]\n",
      "  [0.21008523 0.4430194  0.18460868]\n",
      "  [0.16587994 0.3900071  0.15456559]]\n",
      "\n",
      " [[0.16579132 0.17801121 0.19739148]\n",
      "  [0.23954833 0.25915617 0.27484244]\n",
      "  [0.17888655 0.19849439 0.21418066]\n",
      "  ...\n",
      "  [0.3008229  0.5289039  0.24364518]\n",
      "  [0.2533786  0.47690803 0.2081931 ]\n",
      "  [0.2209729  0.43342012 0.18891783]]]\n",
      "Label:  tf.Tensor(b'rabbit', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# source: https://www.tensorflow.org/tutorials/load_data/images\n",
    "#randomly take one image and see the image and its label\n",
    "for image, label in labeled_ds.take(1):\n",
    "  print(\"Image shape: \", image.numpy())\n",
    "  print(\"Label: \", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeled_ds.batch(batch_size=10, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO\n",
    "def prepare_for_training(dataset,shuffle=False, BATCH_SIZE = 100 ):\n",
    "    \"this is a function that prepares the dataset for training\"\n",
    "    \"input: tensor containing the labeled dataset\"\n",
    "    \"output: dataset, shuffled and batched\"\n",
    "    \n",
    "    if shuffle:\n",
    "        #shuffle the whole dataset: \n",
    "        #for perfect shuffling, buffer should be >= size of ds\n",
    "        ds = labeled_ds.shuffle(buffer_size=3732)\n",
    "    \n",
    "    #repeat forever\n",
    "    #ds = ds.repeat()\n",
    "    \n",
    "    #batch it \n",
    "    #ds = ds.batch(batch_size=BATCH_SIZE, drop_remainder=True)\n",
    "    \n",
    "    #prefetch lets the dataset fetch batched in the background\n",
    "    #while the model is training \n",
    "    #ds = ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    \n",
    "    return ds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'ds' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-68968d1e5530>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabeled_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-d278e12ff92c>\u001b[0m in \u001b[0;36mprepare_for_training\u001b[0;34m(dataset, shuffle, BATCH_SIZE)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'ds' referenced before assignment"
     ]
    }
   ],
   "source": [
    "#NO\n",
    "ds = prepare_for_training(labeled_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT TRAIN,VAL,TEST\n",
    "DATASET_SIZE=37322\n",
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "full_dataset = labeled_ds\n",
    "raw_train = full_dataset.take(train_size)\n",
    "test_dataset = full_dataset.skip(train_size)\n",
    "raw_validation = test_dataset.skip(test_size)\n",
    "raw_test = test_dataset.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((448, 448, 3), ()), types: (tf.float32, tf.string)>\n",
      "<DatasetV1Adapter shapes: ((448, 448, 3), ()), types: (tf.float32, tf.string)>\n",
      "<DatasetV1Adapter shapes: ((448, 448, 3), ()), types: (tf.float32, tf.string)>\n"
     ]
    }
   ],
   "source": [
    "print(raw_train)\n",
    "print(raw_validation)\n",
    "print(raw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FORMAT DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 448\n",
    "# All images will be resized to 160x160\n",
    "\n",
    "def format_example(image, label):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = (image/127.5) - 1\n",
    "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "  return image, label\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = raw_train.map(format_example)\n",
    "validation = raw_validation.map(format_example)\n",
    "test = raw_test.map(format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "validation_batches = validation.batch(BATCH_SIZE)\n",
    "test_batches = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 448, 448, 3])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for image_batch, label_batch in train_batches.take(1):\n",
    "   pass\n",
    "\n",
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.vgg19.VGG19(include_top=False, \n",
    "                                               weights='imagenet', \n",
    "                                               input_tensor=None, \n",
    "                                               input_shape=None,\n",
    "                                               pooling=None, \n",
    "                                               classes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 14, 14, 512)\n"
     ]
    }
   ],
   "source": [
    "feature_batch = base_model(image_batch)\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FREEZING\n",
    "\n",
    "base_model.trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
